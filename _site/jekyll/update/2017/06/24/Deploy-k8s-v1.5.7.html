<!DOCTYPE html>
<html>

  <link
  rel="stylesheet"
  href="/public/css/gpgc_styles.css">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Kubernetes v1.5.7 Deployment</title>
  <meta name="description" content="k8s部署拓扑图">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://localhost:4000/jekyll/update/2017/06/24/Deploy-k8s-v1.5.7.html">
  <link rel="alternate" type="application/rss+xml" title="Hippopo" href="http://localhost:4000/feed.xml">
</head>


  <body>
      <div class="outer">
        <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Hippopo</a>
    
  </div>

  <div class="header-badge">
    <a href="/">
        <img src="/static/img/header.jpg" />
    </a>
  </div>
    
</header>


        <div class="page-content">
          <div class="wrapper">
            <article class="post card" itemscope itemtype="http://schema.org/BlogPosting">
 <div class="card-content">
  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Kubernetes v1.5.7 Deployment</h1>
    <p class="post-meta"><time datetime="2017-06-24T01:01:19+08:00" itemprop="datePublished">Jun 24, 2017</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <h2 id="k8s部署拓扑图">k8s部署拓扑图</h2>

<div>
	<img src="https://tsui89.github.io/static/posts/dk8s/dk8s-topology.jpg" weight="800" height="400" />
</div>

<h2 id="masteretcd">Master+ETCD</h2>

<p>准备工作：</p>

<p><a href="https://pan.baidu.com/s/1nvKBgb3">download dk8s.tar.gz </a>  百度盘 密码: 478m</p>

<p>基础环境准备：需要在所有master/minion节点上执行</p>

<div class="highlighter-rouge"><pre class="highlight"><code>1. download dk8s.tar.gz
2. tar zxvf  dk8s.tar.gz
3. cp -r dk8s/local /root/ 
4. export PATH=$PATH:/root/local/bin
</code></pre>
</div>
<p>注：</p>

<ol>
  <li>以下操作是ubuntu16 systemctl配置方式，ubuntu14请按照修改项修改script里面的脚本启动。</li>
  <li>以下所有kubectl命令都是在master上执行的。</li>
</ol>

<h3 id="搭建单节点etcd">搭建单节点etcd</h3>

<p>dk8s/master/etcd/etcd.service 内容如下：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target
Documentation=https://github.com/coreos

[Service]
Type=notify
WorkingDirectory=/var/lib/etcd/
ExecStart=/root/local/bin/etcd \
  --name=etcd-host0 \
  --initial-advertise-peer-urls=http://192.168.56.135:2380 \
  --listen-peer-urls=http://192.168.56.135:2380 \
  --listen-client-urls=http://192.168.56.135:2379,http://127.0.0.1:2379 \
  --advertise-client-urls=http://192.168.56.135:2379 \
  --initial-cluster-token=etcd-cluster-0 \
  --initial-cluster=etcd-host0=http://192.168.56.135:2380 \
  --initial-cluster-state=new \
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target

</code></pre>
</div>

<p>修改etcd ip地址之后,执行</p>

<div class="highlighter-rouge"><pre class="highlight"><code>mkdir /var/lib/etcd  #创建etcd data-dir
cp dk8s/master/etcd/etcd.service /etc/systemctl/system/
systemctl daemon-reload
systemctl enable etcd
systemctl start etcd
</code></pre>
</div>

<h3 id="启动kube-apiserver">启动kube-apiserver</h3>

<p>dk8s/master/kube-apiserver/kube-apiserver.service 内容如下：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
ExecStart=/root/local/bin/kube-apiserver \
  --admission-control= \
  --advertise-address=192.168.56.135 \
  --insecure-bind-address=0.0.0.0 \
  --service-cluster-ip-range=10.254.0.0/16 \
  --service-node-port-range=30000-60000 \
  --etcd-servers=http://192.168.56.135:2379 \
  --enable-swagger-ui=true \
  --allow-privileged=true \
  --apiserver-count=3 \
  --audit-log-maxage=30 \
  --audit-log-maxbackup=3 \
  --audit-log-maxsize=100 \
  --audit-log-path=/var/lib/audit.log \
  --event-ttl=1h \
  --v=2
Restart=on-failure
RestartSec=5
Type=notify
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
</code></pre>
</div>
<p>修改：</p>

<ul>
  <li>–advertise-address=本机ip</li>
  <li>–etcd-servers=http://&lt;etcd ip&gt;:&lt;port&gt;</li>
</ul>

<div class="language-shell highlighter-rouge"><pre class="highlight"><code>cp dk8s/master/kube-apiserver/kube-apiserver.service /etc/systemctl/system/
systemctl daemon-reload
systemctl <span class="nb">enable </span>kube-apiserver
systemctl start kube-apiserver
</code></pre>
</div>
<p>这时netstat -nltp 能看到8080端口kube-apiserver服务已经启动</p>

<h3 id="启动kube-scheduler">启动kube-scheduler</h3>

<p>dk8s/master/kube-scheduler/kube-scheduler.service 内容如下：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
ExecStart=/root/local/bin/kube-scheduler \
  --address=127.0.0.1 \
  --master=http://192.168.56.135:8080 \
  --leader-elect=true \
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
</code></pre>
</div>

<p>修改：</p>

<ul>
  <li>–master=http://&lt;apiserver ip&gt;:&lt;port&gt;</li>
</ul>

<div class="language-shell highlighter-rouge"><pre class="highlight"><code>cp dk8s/master/kube-scheduler/kube-scheduler.service /etc/systemctl/system/
systemctl daemon-reload
systemctl <span class="nb">enable </span>kube-scheduler
systemctl start kube-scheduler
</code></pre>
</div>

<h3 id="启动kube-controller-manager">启动kube-controller-manager</h3>

<p>dk8s/master/kube-controller/kube-controller-manager.service 内容如下：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
ExecStart=/root/local/bin/kube-controller-manager \
  --address=127.0.0.1 \
  --master=http://192.168.56.135:8080 \
  --allocate-node-cidrs=true \
  --service-cluster-ip-range=10.254.0.0/16 \
  --cluster-cidr=172.30.0.0/16 \
  --cluster-name=kubernetes \
  --leader-elect=true \
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
</code></pre>
</div>
<p>修改:</p>

<ul>
  <li>–master=http://&lt;apiserver ip&gt;:&lt;port&gt;</li>
  <li>–cluster-cidr=Pod网段（这个cidr和两个服务有关kube-controller-manager、calico.yaml；而kube-proxy的cluster-cidr==service-cluster-ip-range）</li>
</ul>

<div class="language-shell highlighter-rouge"><pre class="highlight"><code>cp dk8s/master/kube-controller/kube-controller-manager.service /etc/systemctl/system/
systemctl daemon-reload
systemctl <span class="nb">enable </span>kube-controller-manager
systemctl start kube-controller-manager
</code></pre>
</div>

<p>至此master服务部署基本完成，bird进程是后面部署calico时创建的，现在不用管.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>root@master:~# kubectl get cs
NAME                 STATUS    MESSAGE              ERROR
controller-manager   Healthy   ok
scheduler            Healthy   ok
etcd-0               Healthy   {"health": "true"}

root@master:~# netstat -nltp
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1118/sshd
tcp        0      0 127.0.0.1:10251         0.0.0.0:*               LISTEN      4623/kube-scheduler
tcp        0      0 192.168.56.135:2379     0.0.0.0:*               LISTEN      2246/etcd
tcp        0      0 127.0.0.1:2379          0.0.0.0:*               LISTEN      2246/etcd
tcp        0      0 127.0.0.1:10252         0.0.0.0:*               LISTEN      2633/kube-controlle
tcp        0      0 192.168.56.135:2380     0.0.0.0:*               LISTEN      2246/etcd
tcp        0      0 0.0.0.0:179             0.0.0.0:*               LISTEN      11156/bird
tcp        0      0 127.0.1.1:53            0.0.0.0:*               LISTEN      1195/dnsmasq
tcp6       0      0 :::22                   :::*                    LISTEN      1118/sshd
tcp6       0      0 :::6443                 :::*                    LISTEN      5289/kube-apiserver
tcp6       0      0 :::8080                 :::*                    LISTEN      5289/kube-apiserver
</code></pre>
</div>

<h2 id="minion">Minion</h2>
<h3 id="启动kubelet">启动kubelet</h3>

<p>dk8s/minion/kubelet/kubelet.service 内容如下：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=docker.service
Requires=docker.service

[Service]
WorkingDirectory=/var/lib/kubelet
ExecStart=/root/local/bin/kubelet \
  --api-servers=http://192.168.56.135:8080 \
  --address=192.168.56.136 \
  --hostname-override=192.168.56.136 \
  --cluster_dns=10.254.0.2 \
  --cluster_domain=cluster.local. \
  --hairpin-mode promiscuous-bridge \
  --allow-privileged=true \
  --serialize-image-pulls=false \
  --logtostderr=true \
  --pod-infra-container-image="visenzek8s/pause-amd64:3.0" \
  --network-plugin=cni \
  --network-plugin-dir=/etc/cni/net.d \
  --v=2
ExecStopPost=/sbin/iptables -A INPUT -s 10.0.0.0/8 -p tcp --dport 4194 -j ACCEPT
ExecStopPost=/sbin/iptables -A INPUT -s 172.16.0.0/12 -p tcp --dport 4194 -j ACCEPT
ExecStopPost=/sbin/iptables -A INPUT -s 192.168.0.0/16 -p tcp --dport 4194 -j ACCEPT
ExecStopPost=/sbin/iptables -A INPUT -p tcp --dport 4194 -j DROP
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
</code></pre>
</div>
<p>修改:</p>

<ul>
  <li>–api-servers=http://&lt;apiserver ip&gt;:&lt;port&gt;</li>
  <li>–address=本机IP</li>
  <li>–hostname-override=本机IP</li>
</ul>

<div class="language-shell highlighter-rouge"><pre class="highlight"><code>apt-get install bridge-utils socat
cp dk8s/minion/kubelet/kubelet.service /etc/systemctl/system/
cp dk8s/master/calico/nsenter /usr/local/bin/
mkdir /var/lib/kubelet
cp -r dk8s/master/calico/cni /opt/

mkdir -p /etc/cni/net.d
cat &gt;/etc/cni/net.d/10-calico.conf <span class="sh">&lt;&lt;EOF
{
    "name": "calico-k8s-network",
    "type": "calico",
    "etcd_endpoints": "http://&lt;ETCD_IP&gt;:&lt;ETCD_PORT&gt;",
    "log_level": "info",
    "ipam": {
        "type": "calico-ipam"
    },
    "policy": {
        "type": "k8s"
    },
    "kubernetes": {
        "kubeconfig": "&lt;/PATH/TO/KUBECONFIG&gt;"
    }
}
EOF

</span>systemctl daemon-reload
systemctl <span class="nb">enable </span>kubelet
systemctl start kubelet
</code></pre>
</div>

<h3 id="启动kube-proxy">启动kube-proxy</h3>

<p>dk8s/minion/kube-proxy/kube-proxy.service 内容如下：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
WorkingDirectory=/var/lib/kube-proxy
ExecStart=/root/local/bin/kube-proxy \
  --master=http://192.168.56.135:8080 \
  --bind-address=192.168.56.136 \
  --hostname-override=192.168.56.136 \
  --cluster-cidr=10.254.0.0/16 \
  --logtostderr=true \
  --v=2
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
</code></pre>
</div>
<p>修改:</p>

<ul>
  <li>–master=http://&lt;apiserver ip&gt;:&lt;port&gt;</li>
  <li>–bind-address=本机IP</li>
  <li>–hostname-owveride=本机IP</li>
  <li>–cluster-cidr 必须与 kube-apiserver 的 –service-cluster-ip-range 选项值一致；kube-proxy 根据 –cluster-cidr 判断集群内部和外部流量，指定 –cluster-cidr 或 –masquerade-all 选项后 kube-proxy 才会对访问 Service IP 的请求做 SNAT；</li>
</ul>

<div class="highlighter-rouge"><pre class="highlight"><code>mkdir /var/lib/kube-proxy
cp kube-proxy.service /etc/systemctl/system/
systemctl daemon-reload
systemctl enable kube-proxy
systemctl start kube-proxy
</code></pre>
</div>
<p>至此minion部署完毕</p>

<p>在master上执行</p>

<div class="highlighter-rouge"><pre class="highlight"><code>root@master:~# kubectl get nodes
NAME             STATUS    AGE
192.168.56.136   Ready     21h
192.168.56.137   Ready     19h
192.168.56.138   Ready     18h
</code></pre>
</div>
<h2 id="部署calico">部署calico</h2>

<p>准备镜像,
在minion节点上</p>

<div class="language-shell highlighter-rouge"><pre class="highlight"><code><span class="nb">cd </span>k8s.images
<span class="k">for </span>f <span class="k">in</span> <span class="k">*</span>.dif;do docker load -i <span class="nv">$f</span>;<span class="k">done
for </span>f <span class="k">in </span>calico/<span class="k">*</span>.dif;do docker load -i <span class="nv">$f</span>;<span class="k">done</span>
</code></pre>
</div>
<p>在master节点上：</p>

<p>dk8s/master/calico/calico.yaml 修改:</p>

<ul>
  <li>etcd_endpoints: “http://192.168.56.135:2379”对应etcd地址</li>
  <li>CALICO_IPV4POOL_CIDR对应kube-controller-manager配置的–cluster-cidr的值</li>
  <li>IP_AUTODETECTION_METHOD指定calico绑定Node的哪一个eth</li>
  <li>CALICO_IPV4POOL_IPIP off</li>
</ul>

<div class="language-shell highlighter-rouge"><pre class="highlight"><code><span class="gp">root@master:~/dk8s/master/calico# </span>kubectl apply -f calico.yaml
<span class="gp">root@master:~/dk8s# </span>kubectl get pods --namespace<span class="o">=</span>kube-system -o wide
NAME                                       READY     STATUS    RESTARTS   AGE       IP               NODE
calico-node-1kpqd                          2/2       Running   0          18h       192.168.56.138   192.168.56.138
calico-node-9bp5d                          2/2       Running   0          18h       192.168.56.136   192.168.56.136
calico-node-js8hk                          2/2       Running   0          18h       192.168.56.137   192.168.56.137
calico-policy-controller-279105993-jlg4w   1/1       Running   0          18h       192.168.56.137   192.168.56.137
</code></pre>
</div>
<p>修改dk8s/master/calico/conf/calico.conf中etcd地址</p>

<div class="language-shell highlighter-rouge"><pre class="highlight"><code><span class="gp">root@master:~/dk8s/master/calico# </span>./get_nodes.sh
NAME      ASN       IPV4                IPV6
minion1   <span class="o">(</span>64512<span class="o">)</span>   192.168.56.136/24
minion2   <span class="o">(</span>64512<span class="o">)</span>   192.168.56.137/24
minion3   <span class="o">(</span>64512<span class="o">)</span>   192.168.56.138/24
</code></pre>
</div>
<p>这时候所有的minion节点都配好calico服务了，为了让master能ping通pod，还需要我们自己手动启动caliconode。
在master上执行</p>

<div class="highlighter-rouge"><pre class="highlight"><code>calicoctl node run -c dk8s/master/calico/conf/calico.conf #可以通过--ip-autodetection-method 参数指定interface
</code></pre>
</div>

<div class="language-shell highlighter-rouge"><pre class="highlight"><code><span class="gp">root@master:~/dk8s/master/calico# </span>./get_nodes.sh
NAME      ASN       IPV4                IPV6
master    <span class="o">(</span>64512<span class="o">)</span>   192.168.56.135/24
minion1   <span class="o">(</span>64512<span class="o">)</span>   192.168.56.136/24
minion2   <span class="o">(</span>64512<span class="o">)</span>   192.168.56.137/24
minion3   <span class="o">(</span>64512<span class="o">)</span>   192.168.56.138/24
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>root@master:~# route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         10.0.2.1        0.0.0.0         UG    100    0        0 enp0s8
10.0.2.0        0.0.0.0         255.255.255.0   U     100    0        0 enp0s8
169.254.0.0     0.0.0.0         255.255.0.0     U     1000   0        0 enp0s3
172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0
172.30.34.64    192.168.56.136  255.255.255.192 UG    0      0        0 enp0s3
172.30.51.128   192.168.56.138  255.255.255.192 UG    0      0        0 enp0s3
172.30.180.0    192.168.56.137  255.255.255.192 UG    0      0        0 enp0s3
172.30.219.64   0.0.0.0         255.255.255.192 U     0      0        0 *
192.168.56.0    0.0.0.0         255.255.255.0   U     0      0        0 enp0s3
</code></pre>
</div>
<p>172.30.0.0/16就是pod获取到的ip网段。</p>

<h2 id="dashboard">Dashboard</h2>

<p>dk8s/master/kube-dashboard/heapster/heapster.yaml修改:</p>

<ul>
  <li>–source=kubernetes:https://kubernetes.default 为 –source=kubernetes:http://&lt;apiserver ip&gt;:&lt;port&gt;?inClusterConfig=false</li>
</ul>

<p>dk8s/master/kube-dashboard/dashboard.yaml、dk8s/master/kube-dashboard/head.yaml修改:</p>

<ul>
  <li>–apiserver-host=http://&lt;apiserver ip&gt;:&lt;port&gt;</li>
</ul>

<div class="highlighter-rouge"><pre class="highlight"><code>kubectl create -f dk8s/master/kube-dashboard/heapster
kubectl create -f dk8s/masterkube-dashboard
</code></pre>
</div>

<h2 id="dns">dns</h2>

<p>dk8s/master/dns/skydns-rc.yaml 修改:</p>

<ul>
  <li>–kube-master-url=http://&lt;apiserver ip&gt;:&lt;port&gt;</li>
</ul>

<div class="highlighter-rouge"><pre class="highlight"><code>kubectl create -f dk8s/master/dns
</code></pre>
</div>

<h2 id="nginx-测试">nginx 测试</h2>

<div class="highlighter-rouge"><pre class="highlight"><code>kubectl create -f dk8s/test-nginx
service "nginx-ds" created
daemonset "nginx-ds" created
</code></pre>
</div>
<p>1.测试service dns</p>

<div class="highlighter-rouge"><pre class="highlight"><code>root@master:~# kubectl exec busybox -- nslookup nginx-ds
Server:    10.254.0.2
Address 1: 10.254.0.2 kube-dns.kube-system.svc.cluster.local

Name:      nginx-ds
Address 1: 10.254.42.90 nginx-ds.default.svc.cluster.local
</code></pre>
</div>

<p>2.测试pod Ip</p>

<div class="highlighter-rouge"><pre class="highlight"><code>root@master:~# kubectl get pods --all-namespaces -o wide |grep nginx-ds
default       nginx-ds-47mwz                               1/1       Running   0          30m       172.30.180.14    192.168.56.137
default       nginx-ds-fft61                               1/1       Running   0          30m       172.30.34.82     192.168.56.136
default       nginx-ds-mt8fx                               1/1       Running   0          30m       172.30.51.149    192.168.56.138

root@master:~# wget 172.30.180.14:80
--2017-06-01 18:59:52--  http://172.30.180.14/
Connecting to 172.30.180.14:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 612 [text/html]
Saving to: ‘index.html’

index.html                                            100%[======================================================================================================================&gt;]     612  --.-KB/s    in 0s      

2017-06-01 18:59:52 (173 MB/s) - ‘index.html’ saved [612/612]


</code></pre>
</div>

<p>3.测试集群内部服务</p>

<div class="highlighter-rouge"><pre class="highlight"><code>root@master:~# kubectl exec busybox -- wget nginx-ds:80
Connecting to nginx-ds (10.254.42.90:80)
index.html           100% |*******************************|   612   0:00:00 ETA

</code></pre>
</div>

<p>4.测试外部服务：浏览器访问任意&lt;node ip&gt;:36666,会显示nginx页面</p>

  </div>
</div>
</article>
<div class="gpgc-new-section gpgc-comments-font gpgc-hidden" id="gpgc_all_comments">
</div>

<div class="gpgc-new-section gpgc-comments-font gpgc-hidden" id="gpgc_no_comments">
  <p class="gpgc-centered-text">No comments</p>
</div>

<div class="gpgc-actions gpgc-hidden gpgc-color-theme" id="gpgc_actions">
  <span class="gpgc-action gpgc-centered-text">
    <button class="gpgc-large-secondary-button" id="show_comments_button" onclick="showAllComments(CommentsArray)"></button>
  </span>
</div>

<div class="gpgc-comment gpgc-comments-font gpgc-color-theme" id="gpgc_new_comment">
  <div class="gpgc-comment-header">
    <a id="gpgc_reader_url" href="https://github.com/wireddown/ghpages-ghcomments"><img class="gpgc-avatar" id="gpgc_reader_avatar" src="https://raw.githubusercontent.com/wireddown/ghpages-ghcomments/gh-pages/public/apple-touch-icon-precomposed.png" height="42" /><span id="gpgc_reader_login">You</span></a>
    <small>today</small>
    <div class="gpgc-tabs">
      <button class="gpgc-tab gpgc-text-button" id="write_button">Write</button>
      <button class="gpgc-tab gpgc-text-button" id="preview_button">Preview</button>
    </div>
  </div>
  <div class="gpgc-new-comment" id="write_div">
    <div class="gpgc-new-comment-form">
      <textarea class="gpgc-new-comment-form-textarea" id="new_comment_field" placeholder="Write a comment"></textarea>
    </div>
  </div>
  <div class="gpgc-new-comment gpgc-comment-contents gpgc-hidden" id="preview_div">
  </div>
  <div class="gpgc-hidden gpgc-comment-help" id="help_message">
  </div>
  <div class="gpgc-new-comment-actions">
    <button class="gpgc-normal-primary-button gpgc-text-button" id="login_button" onclick="loginToGitHub()"><strong>Login to GitHub</strong></button>
    <button class="gpgc-normal-primary-button gpgc-text-button gpgc-hidden" id="submit_button" onclick="postComment()"><strong>Submit</strong></button>
  </div>
</div>

<div class="gpgc-new-section gpgc-comments-font gpgc-hidden" id="gpgc_disabled">
  <p class="gpgc-centered-text">Comments are closed</p>
</div>

<div class="gpgc-new-section gpgc-comments-font gpgc-help-error gpgc-hidden" id="gpgc_reader_error">
</div>

<div class="gpgc_last_div">
</div>

<script type="text/javascript" src="/public/js/gpgc_core.js"></script>

<script>
  window.addEventListener("error", onError);
  function onError(error) {
    var errorMessage = "<h3><strong>gpgc</strong> Error: Global Exception</h3>";
    errorMessage += "<p><dl><dt>Error</dt><dd>" + error.message + "</dd><dt>URL</dt><dd>" + error.filename + "</dd><dt>Line</dt><dd>" + error.lineno + "</dd><dt>Column</dt><dd>" + error.colno + "</dd></dl></p>"
    showGeneralHelp(errorMessage);
  }
</script>

<script>
  var gpgc = {
    new_comments_disabled: new Boolean(false).valueOf() || new Boolean().valueOf(),
    site_url: "https://tsui89.github.io",
    page_path: "_posts/2017-06-25-Deploy-k8s-v1.5.7.markdown",
    issue_title: "Kubernetes v1.5.7 Deployment",
    repo_id: "tsui89/tsui89.github.io",
    use_show_action: false,
    github_application_client_id: "8b8dc39ff561c72d08ec",
    github_application_code_authenticator_url: "https://hippopo.herokuapp.com/authenticate/",
    github_application_login_redirect_url: "http://tsui89.github.io/public/html/gpgc_redirect/index.html",
    enable_diagnostics: true,
  };

  gpgc_main();
</script>


          </div>
        </div>

        <footer class="site-footer">

  <div class="wrapper">

    <span class="footer-heading">Hippopo</span>

  </div>

</footer>

      </div>
  </body>
  
</html>
